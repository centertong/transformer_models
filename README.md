# Transformers Test
Linear Complexity Transformers Test using KLUE dataset

## Models
* BERT (From huggingface transformers library)
* Performer (From https://github.com/lucidrains/performer-pytorch)
* RFA (Random Feature Attention) [Only 'arccos' mode]
* Lite Transformer
* Attention Free Transformer
* Luna: Linear Unified Nested Attention
* ScatterBrain [work in progress`]

## Reference
1. hugginface transformers
2. hugginface tokenizers
3. performer-pytorch
