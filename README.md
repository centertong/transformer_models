# Transformers Test
Linear Complexity Transformers Test using KLUE dataset

# Models
* BERT (From huggingface transformers library)
* Performer (From https://github.com/lucidrains/performer-pytorch)
* RFA (Random Feature Attention)
* Lite Transformer [Modifying]
* Attention Free Transformer [Modifying]

# Reference
1. hugginface transformers
2. hugginface tokenizers
3. performer-pytorch
